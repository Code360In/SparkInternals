{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Understand dag in spark web ui\n",
    "\n",
    "In this section, we will learn how to read the DAG in the spark web ui\n",
    "\n",
    "As you know, spark uses lazy execution model, then you run a transformation, there will be no actual job that execute the transformation. A **spark job** is triggered until you execute an action (e.g. show, collect, take, count, etc.)\n",
    "\n",
    "\n",
    "When a spark job is executed, In spark web UI (Jobs tab), you will find the job grouped by its status (e.g. running, succeed, failed.). Click on the job description, you will land on the **details for job** page. In this page, you can view the execution DAG of all stages. **A DAG represents a chain of RDD transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql import functions as f"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/02 15:54:23 WARN Utils: Your hostname, pliu-SATELLITE-P850 resolves to a loopback address: 127.0.1.1; using 172.22.0.33 instead (on interface wlp3s0)\n",
      "22/03/02 15:54:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/spark-3.1.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/02 15:54:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "local = True\n",
    "\n",
    "if local:\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"UnderstandSparkUIDAG\") \\\n",
    "        .getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "        .appName(\"UnderstandSparkUIDAG\") \\\n",
    "        .config(\"spark.kubernetes.container.image\", \"inseefrlab/jupyter-datascience:py3.9.7-spark3.2.0\")\\\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", os.environ['KUBERNETES_SERVICE_ACCOUNT'])\\\n",
    "        .config(\"spark.executor.instances\", \"4\")\\\n",
    "        .config(\"spark.executor.memory\", \"8g\")\\\n",
    "        .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\\\n",
    "        .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.range(1,100000)\n",
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Job details\n",
    "After running the above code, open your spark UI (if you run in local mode, the default url is http://localhost:4040/jobs/). You should see a job, click on it, you should see a page like below figure\n",
    "![Spark_UI_JOB_details](../img/spark_ui_job_details.png)\n",
    "\n",
    "In the **event timeline**, you can check the lifecycle of each executor (e.g. added, removed) and stages of this job. Executors are per application, when an application is terminated, all executors belongs to this application will be removed. In cluster mode, each executor is an individual jvm process.\n",
    "The above figure shows the timeline of local mode, you can notice, there is only one driver executor. If we run it in cluster mode, we\n",
    "will have 4 executor (configured in spark session creation). Below figure is an example\n",
    " ![Spark_UI_JOB_timeline](../img/spark_ui_job_timeline.png)\n",
    "\n",
    "\n",
    "In the **DAG Visualization**, the **blue shaded boxes represent to the Spark operation** that the user calls in the code. The **dots in these boxes represent RDDs created in the corresponding operations**.\n",
    "\n",
    "\n",
    "In our case, we have two boxes. The name of the first box is **WholeStageCodegen**, because we create a dataframe that involves java code generation to build underlying RDDs. Then second box is **mapPartitionsInternal**, because action show() collects data over each of the RDD's partitions\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stage details\n",
    "\n",
    "If you want to have more details of a stage, you just click on that stage. You will see the following stage details page:\n",
    "\n",
    "![spark_ui_stage_details](../img/spark_ui_stage_details.png)\n",
    "\n",
    "And the event timeline can tell you how much time each step of the stage spent. Below figure is an example:\n",
    "\n",
    "![spark_ui_stage_details_et](../img/spark_ui_stage_details_et.png)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spark DAG stage operation name\n",
    "- **WholeStageCodegen**: happens when you run computations on DataFrames and generates Java code to build underlying RDDs\n",
    "- **mapPartitions**: happens when you run computation over each of the RDD's partitions (parallelization friendly, high performance)\n",
    "-"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Shuffle operations\n",
    "\n",
    "Below command will create a dataframe first (with default partition number), then we change the partition number of the dataframe to 8. To trigger the run of the transformation, we use take(2) to return two random rows of the dataframe\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  3|\n",
      "|  5|\n",
      "|  7|\n",
      "|  9|\n",
      "| 11|\n",
      "| 13|\n",
      "| 15|\n",
      "| 17|\n",
      "| 19|\n",
      "| 21|\n",
      "| 23|\n",
      "| 25|\n",
      "| 27|\n",
      "| 29|\n",
      "| 31|\n",
      "| 33|\n",
      "| 35|\n",
      "| 37|\n",
      "| 39|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "[Row(id=221717), Row(id=153517)]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=spark.range(1,1000000,2)\n",
    "df1.show()\n",
    "df1_split7=df1.repartition(8)\n",
    "df1_split7.take(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, you should see the following DAG:\n",
    "![spark_ui_job_shuffle](../img/spark_ui_job_shuffle.png)\n",
    "\n",
    "Now you can notice, this time we have two stages inside the job. And we have a new type of operation called **exchange**. This operation is caused by a **shuffle**, is an operation in which data is exchanged (hence the name of the operation) between all the executors in the cluster. The more massive your data and your cluster is, the more expensive this shuffle will be, because sending data over network takes time. When you optimize your spark job, you just eliminate **shuffle** as much as possible.\n",
    "\n",
    "Spark decomposes a job into stages by using shuffles, when spark encounter a shuffle, it will create a new stage.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A complete example\n",
    "\n",
    "With below example, we create two dataframe and change their partition. Then we join the two dataframe. And finally we execute an aggregation function on the joined dataframe."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:=============================================>        (170 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      sum(id)|\n",
      "+-------------+\n",
      "|5000000000000|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ds1 = spark.range(1, 10000000)\n",
    "ds2 = spark.range(1, 10000000, 2)\n",
    "\n",
    "ds3 = ds1.repartition(7)\n",
    "\n",
    "ds4 = ds2.repartition(9)\n",
    "\n",
    "ds5 = ds3.selectExpr(\"id * 5 as id\")\n",
    "\n",
    "joined = ds5.join(ds4, \"id\")\n",
    "\n",
    "sum = joined.selectExpr(\"sum(id)\")\n",
    "\n",
    "sum.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above queries will generate the following DAG:\n",
    "![spark_ui_job_complexe](../img/spark_ui_job_complexe.png)\n",
    "\n",
    "- Stage 21: represent ds1 = spark.range(1, 10000000)\n",
    "- Stage 22: represent ds2 = spark.range(1, 10000000, 2)\n",
    "- Stage 23: represent ds4 = ds2.repartition(9),  here we have a new stage because repartition requires a shuffle, and we only re-organize the data, so we only have exchange operations in this stage\n",
    "- Stage 24: represent ds3 = ds1.repartition(7), ds5 = ds3.selectExpr(\"id * 5 as id\"), you can notice we have a WholeStageCodegen operation between two exchange operation. Because \"id * 5 as id\" requires generation of java code to do calculation over RDDs.\n",
    "- Stage 25: represent joined = ds5.join(ds4, \"id\"). Join also requires a shuffle, because it needs to place the rows which have the same key on the same executor to do the join.\n",
    "- Stage 26: represents sum = joined.selectExpr(\"sum(id)\"). The shuffle is caused by the sum(). Because Spark will need to bring all the data to a single executor in order to perform the final sum. So be careful on aggregation functions, they usually involve some form of moving data between executors (aka. shuffle).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cache data\n",
    "\n",
    "Spark does not save intermediate RDD/dataframe/DS, so when you use a intermediate dataframe, spark loads data from source and repeat the calculation each time. To avoid this, you can use cache() or persist() to store intermediate RDD/DF/DS.\n",
    "\n",
    "The difference is, by default cache() method saves RDD to memory (MEMORY_ONLY) whereas persist() method is used to store it to the user-defined storage level."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/02 16:09:17 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "df=spark.range(1,100000)\n",
    "df.cache()\n",
    "df_persist=df.select((f.col(\"id\")*5).alias(\"id\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cached RDD is represented by a green dot/rectangle. Below figure shows the above cache dataframe in DAG.\n",
    "![spark_ui_cache](../img/spark_ui_cache.png)\n",
    "\n",
    "You can also find the cached dataframe in storage tab. Below figure is an example\n",
    "![spark_ui_storage](../img/spark_ui_storage.png)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Persist\n",
    "Persist is similar to the cache, you only need to add a storage level.\n",
    "\n",
    "A storage level can have following possible values:\n",
    "- MEMORY_ONLY,\n",
    "- MEMORY_AND_DISK,\n",
    "- MEMORY_ONLY_SER,\n",
    "- MEMORY_AND_DISK_SER,\n",
    "- DISK_ONLY,\n",
    "- MEMORY_ONLY_2,\n",
    "- MEMORY_AND_DISK_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  5|\n",
      "| 10|\n",
      "| 15|\n",
      "| 20|\n",
      "| 25|\n",
      "| 30|\n",
      "| 35|\n",
      "| 40|\n",
      "| 45|\n",
      "| 50|\n",
      "| 55|\n",
      "| 60|\n",
      "| 65|\n",
      "| 70|\n",
      "| 75|\n",
      "| 80|\n",
      "| 85|\n",
      "| 90|\n",
      "| 95|\n",
      "|100|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "df_persist.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "df_persist.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[id: bigint]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can unpersist a cached RDD/DF/DS with the following command\n",
    "df_persist.unpersist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[id: bigint]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can unpersist a cached RDD/DF/DS with the following command\n",
    "# You can consider cache is just a wrapper of persist(StorageLevel.MEMORY_ONLY)\n",
    "df.unpersist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}