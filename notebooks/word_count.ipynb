{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35f26109-0e1d-4499-a7c1-1441ff065d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74dc00d0-3c1d-4f05-ab13-91d93a310603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a6f52caf-57c6-408d-ac69-11edc316e438;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.2.24 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      ":: resolution report :: resolve 176ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.2.24 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a6f52caf-57c6-408d-ac69-11edc316e438\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/5ms)\n",
      "2021-10-25 11:59:44,020 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "local=False\n",
    "if local:\n",
    "    spark=SparkSession.builder.master(\"local[4]\") \\\n",
    "                  .config('spark.jars.packages', 'org.postgresql:postgresql:42.2.24') \\\n",
    "                  .appName(\"RemoveDuplicates\").getOrCreate()\n",
    "else:\n",
    "    spark=SparkSession.builder \\\n",
    "                      .master(\"k8s://https://kubernetes.default.svc:443\") \\\n",
    "                      .appName(\"RemoveDuplicates1\") \\\n",
    "                      # the image of the worker must be the same as the driver image.  \n",
    "                      .config(\"spark.kubernetes.container.image\",\"inseefrlab/jupyter-datascience:py3.9.7-spark3.1.2\") \\\n",
    "                      # add the driver pod name, to auto-terminate executor pod after driver pod is deleted \n",
    "                      .config(\"spark.kubernetes.driver.pod.name\", os.environ['KUBERNETES_POD_NAME'])\n",
    "                      .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\",os.environ['KUBERNETES_SERVICE_ACCOUNT']) \\\n",
    "                      .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE']) \\\n",
    "                      .config(\"spark.executor.instances\", \"4\") \\\n",
    "                      .config(\"spark.executor.memory\",\"8g\") \\\n",
    "                      .config('spark.jars.packages','org.postgresql:postgresql:42.2.24') \\\n",
    "                      .getOrCreate()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014887cf-070d-4138-8f4a-a3aa5b29096a",
   "metadata": {},
   "source": [
    "## Client Mode Executor Pod Garbage Collection\n",
    "\n",
    "If you run your Spark driver in a k8s pod (as in this example), it is highly recommended to set **spark.kubernetes.driver.pod.name to the name of that pod**. When this property is set, the Spark scheduler will deploy the executor pods with an OwnerReference, which in turn will ensure that once the driver pod is deleted from the cluster, all of the application’s executor pods will also be deleted. The driver will look for a pod with the given name in the namespace specified by spark.kubernetes.namespace, and an OwnerReference pointing to that pod will be added to each executor pod’s OwnerReferences list. \n",
    "\n",
    "**Be careful to avoid setting the OwnerReference to a pod that is not actually that driver pod, or else the executors may be terminated prematurely when the wrong pod is deleted.**\n",
    "\n",
    "In this example, the pod who runs this notebook is the driver pod. So in the config we add the following line\n",
    "\n",
    "``` python\n",
    "# The driver pod name is the current pod name.\n",
    ".config(\"spark.kubernetes.driver.pod.name\", os.environ['KUBERNETES_POD_NAME'])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7133bd-474f-4fa6-8185-08e211839d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1025 11:59:54.054167    1677 request.go:665] Waited for 1.168526028s due to client-side throttling, not priority and fairness, request: GET:https://kubernetes.default/apis/apiextensions.k8s.io/v1beta1?timeout=32s\n",
      "NAME                                               READY   STATUS      RESTARTS   AGE\n",
      "amundsen-elasticsearch-master-0                    1/1     Running     0          5d1h\n",
      "amundsen-elasticsearch-master-1                    1/1     Running     0          5d1h\n",
      "amundsen-elasticsearch-master-2                    1/1     Running     0          5d1h\n",
      "amundsen-test-frontend-5648d65454-ffgpt            1/1     Running     0          5d1h\n",
      "amundsen-test-metadata-c57c47666-7bjsz             1/1     Running     0          5d1h\n",
      "amundsen-test-search-5c8455d784-g56vh              1/1     Running     0          5d1h\n",
      "datahub-datahub-frontend-6ff5494f44-7gcpx          1/1     Running     0          4d20h\n",
      "datahub-datahub-gms-5b66978446-g2526               1/1     Running     0          4d20h\n",
      "datahub-datahub-upgrade-job-spgg2                  0/1     Error       0          4d20h\n",
      "datahub-datahub-upgrade-job-vzb8g                  0/1     Completed   0          4d20h\n",
      "datahub-elasticsearch-setup-job-j2hhc              0/1     Completed   0          4d20h\n",
      "datahub-kafka-setup-job-knrjt                      0/1     Completed   0          4d20h\n",
      "datahub-mysql-setup-job-7tqzl                      0/1     Completed   0          4d20h\n",
      "elasticsearch-master-0                             1/1     Running     0          4d20h\n",
      "elasticsearch-master-1                             1/1     Running     0          4d20h\n",
      "elasticsearch-master-2                             1/1     Running     0          4d20h\n",
      "flume-test-agent-df8c5b944-vtjbx                   1/1     Running     0          35d\n",
      "jupyter-43074-6d44c5d9cd-hpfhx                     1/1     Running     0          14m\n",
      "jupyter-483663-5cf687c876-tpjq6                    1/1     Running     0          25h\n",
      "neo4j-84cc979d5f-vjcc8                             1/1     Running     0          5d1h\n",
      "postgresql-583865-postgresql-0                     1/1     Running     0          7d2h\n",
      "prerequisites-cp-schema-registry-cf79bfccf-6hz6x   2/2     Running     1          4d20h\n",
      "prerequisites-kafka-0                              1/1     Running     2          4d20h\n",
      "prerequisites-mysql-0                              1/1     Running     0          4d20h\n",
      "prerequisites-neo4j-community-0                    1/1     Running     0          4d20h\n",
      "prerequisites-zookeeper-0                          1/1     Running     0          4d20h\n",
      "removeduplicates-83d35c7cb217cbc0-exec-1           1/1     Running     0          24h\n",
      "removeduplicates-83d35c7cb217cbc0-exec-2           1/1     Running     0          24h\n",
      "removeduplicates-83d35c7cb217cbc0-exec-3           1/1     Running     0          24h\n",
      "removeduplicates-83d35c7cb217cbc0-exec-4           1/1     Running     0          24h\n",
      "removeduplicates1-5878367cb75220f9-exec-1          1/1     Running     0          12s\n",
      "removeduplicates1-5878367cb75220f9-exec-2          1/1     Running     0          12s\n",
      "removeduplicates1-5878367cb75220f9-exec-3          1/1     Running     0          12s\n",
      "removeduplicates1-5878367cb75220f9-exec-4          1/1     Running     0          12s\n",
      "ubuntu-978925-6fc97f8db6-5bsgq                     1/1     Running     0          6d23h\n",
      "vscode-683279-675c7d8fdf-xqfmt                     1/1     Running     0          7d4h\n",
      "vscode-882919-7bbb5997d9-6dp7j                     1/1     Running     0          4d21h\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53445852-dcc2-4a7e-911e-a696929848a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[\"hadoop spark\",\"hadoop flume\",\"spark kafka\",\"hello spark\"]\n",
    "textRdd=spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4254dbb-92a6-43c6-8325-efa4ddfd5f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitRdd=textRdd.flatMap(lambda item: item.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "244dcfad-690a-465c-9d9d-2507a651bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tupleRdd=splitRdd.map(lambda item: (item,1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e57111f5-9d49-4bb0-8f11-32854754b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduceRdd=tupleRdd.reduceByKey(lambda x,y:x+y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bd31ed4-87cb-4ef0-b579-5ee283c17963",
   "metadata": {},
   "outputs": [],
   "source": [
    "strRdd=reduceRdd.map(lambda item: f\"{item[0]}, {item[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fe1f303-d6d9-439f-8696-cc9d69386a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop, 2\n",
      "spark, 3\n",
      "hello, 1\n",
      "flume, 1\n",
      "kafka, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "list=strRdd.collect()\n",
    "for item in list:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf548d-849f-45c8-89be-cced873e2fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "strRdd.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d8d08-8986-4168-8a06-22e5207dd163",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
